import time
import json
import os
import re
import requests
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options

# ==========================================
# 1. ì±”í”¼ì–¸ ì´ë¦„ ì˜ˆì™¸ ì²˜ë¦¬ (Lolalytics URL ê·œì¹™)
# ==========================================
URL_EXCEPTIONS = {
    "Renata": "renata",
    "MonkeyKing": "wukong",
    "Nunu": "nunu",
    "DrMundo": "drmundo",
    "JarvanIV": "jarvaniv",
    "LeeSin": "leesin",
    "MasterYi": "masteryi",
    "MissFortune": "missfortune",
    "TahmKench": "tahmkench",
    "TwistedFate": "twistedfate",
    "XinZhao": "xinzhao",
    "KogMaw": "kogmaw",
    "RekSai": "reksai",
    "Belveth": "belveth",
    "Glasc": "renata" 
}

def get_champion_list():
    """ë¼ì´ì—‡ APIì—ì„œ ìµœì‹  ì±”í”¼ì–¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    try:
        ver_url = "https://ddragon.leagueoflegends.com/api/versions.json"
        version = requests.get(ver_url).json()[0]
        champ_url = f"https://ddragon.leagueoflegends.com/cdn/{version}/data/ko_KR/champion.json"
        data = requests.get(champ_url).json()
        return list(data['data'].keys())
    except Exception as e:
        print(f"âš ï¸ ì±”í”¼ì–¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨ (ê¸°ë³¸ê°’ ì‚¬ìš©): {e}")
        return ["Gwen", "Ezreal", "Ahri"] 

def get_slug(champ_id):
    """ì±”í”¼ì–¸ IDë¥¼ Lolalytics URL ìŠ¬ëŸ¬ê·¸ë¡œ ë³€í™˜"""
    if champ_id in URL_EXCEPTIONS: return URL_EXCEPTIONS[champ_id]
    return champ_id.lower().replace(" ", "").replace("'", "").replace(".", "")

def extract_id_from_url(url):
    """URLì—ì„œ ì•„ì´í…œ ID ìˆ«ìë§Œ ì¶”ì¶œ (ì˜ˆ: .../1001.webp -> 1001)"""
    if not url: return None
    match = re.search(r'/(\d+)\.webp', url)
    return int(match.group(1)) if match else None

# ==========================================
# 2. ì‹œì‘ ì•„ì´í…œ íŒŒì‹± í•¨ìˆ˜ (ìˆ˜ì •ë¨)
# ==========================================
def parse_starting_items(driver):
    """
    [ìˆ˜ì •] ì‹œì‘ ì•„ì´í…œì€ í•˜ë‚˜ì˜ ë°•ìŠ¤ ì•ˆì— ì—¬ëŸ¬ ì•„ì´í…œì´ ìˆê³ ,
    ìŠ¹ë¥ /ê²Œì„ ìˆ˜ëŠ” ë°•ìŠ¤ ë§¨ ì•„ë˜ì— ë‹¨ í•˜ë‚˜ë§Œ ì¡´ì¬í•¨.
    """
    items = []
    try:
        # 1. "Starting Items" í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ ìµœìƒìœ„ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
        xpath = "//div[contains(text(), 'Starting Items')]/ancestor::div[contains(@class, 'basis')]"
        containers = driver.find_elements(By.XPATH, xpath)

        for container in containers:
            # 2. í†µê³„ ì •ë³´ ë¨¼ì € ì¶”ì¶œ (ë°•ìŠ¤ í•˜ë‹¨ì— ìˆìŒ)
            win_rate = ""
            games = ""
            try:
                wr_elem = container.find_element(By.CSS_SELECTOR, "span.text-green-500")
                win_rate = wr_elem.text.replace("%", "").replace(" Win Rate", "").strip()
            except: pass

            try:
                g_elem = container.find_element(By.CSS_SELECTOR, "span.text-gray-400")
                games = g_elem.text.replace(" Games", "").strip()
            except: pass

            # 3. ì•„ì´í…œ ì´ë¯¸ì§€ë“¤ ì¶”ì¶œ
            # (ì¤‘ìš”: íˆ´íŒìš© ì‘ì€ ì´ë¯¸ì§€ê°€ ì•„ë‹ˆë¼ ë©”ì¸ ì´ë¯¸ì§€ë¥¼ ì°¾ì•„ì•¼ í•¨. ë³´í†µ h-[34px] ì•ˆì— ìˆìŒ)
            imgs = container.find_elements(By.CSS_SELECTOR, "div.h-\\[34px\\] img")
            
            for img in imgs:
                src = img.get_attribute("src")
                item_id = extract_id_from_url(src)
                
                if item_id:
                    # ëª¨ë“  ì•„ì´í…œì— ë™ì¼í•œ ìŠ¹ë¥  ì ìš©
                    items.append({
                        "id": item_id,
                        "win": win_rate,
                        "games": games
                    })
    except Exception as e:
        pass # ì„¹ì…˜ì´ ì—†ì„ ìˆ˜ë„ ìˆìŒ
        
    return items

# ==========================================
# 3. ì¼ë°˜ ì„¹ì…˜ íŒŒì‹± í•¨ìˆ˜ (ì½”ì–´, 4, 5, 6 ì•„ì´í…œ)
# ==========================================
def parse_section(driver, header_text):
    """
    ì¼ë°˜ ì„¹ì…˜: ì•„ì´í…œë³„ë¡œ ìŠ¹ë¥ ì´ ë”°ë¡œ ë¶™ì–´ìˆëŠ” ê²½ìš° (Item 4, 5, 6 ë“±)
    ë˜ëŠ” ì½”ì–´ ë¹Œë“œì²˜ëŸ¼ ìˆœì„œëŒ€ë¡œ ë‚˜ì—´ëœ ê²½ìš°
    """
    items = []
    try:
        # í•´ë‹¹ í—¤ë”ë¥¼ ê°€ì§„ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
        xpath = f"//div[contains(text(), '{header_text}')]/ancestor::div[contains(@class, 'basis')]"
        container = driver.find_element(By.XPATH, xpath)
        
        # ì´ë¯¸ì§€ì™€ í†µê³„ê°€ ë¬¶ì—¬ìˆëŠ” ë¸”ë¡ë“¤ ì°¾ê¸° (text-center í´ë˜ìŠ¤ í•˜ìœ„)
        # ë³´í†µ êµ¬ì¡°: div.text-center > div.overflow-hidden (ì´ë¯¸ì§€) + span (í†µê³„)
        blocks = container.find_elements(By.CSS_SELECTOR, "div.text-center")

        for block in blocks:
            try:
                # ì´ë¯¸ì§€ ì°¾ê¸° (ì—†ìœ¼ë©´ íŒ¨ìŠ¤)
                try:
                    img = block.find_element(By.TAG_NAME, "img")
                except:
                    continue

                src = img.get_attribute("src")
                item_id = extract_id_from_url(src)
                if not item_id: continue

                # í†µê³„ ì¶”ì¶œ (ì´ë¯¸ì§€ì™€ í˜•ì œ ë…¸ë“œê±°ë‚˜ ë¶€ëª¨ì˜ í˜•ì œì¼ ìˆ˜ ìˆìŒ)
                win_rate = ""
                games = ""
                
                # ë¸”ë¡ ì•ˆì—ì„œ ë°”ë¡œ ì°¾ê¸° ì‹œë„
                try:
                    win_rate = block.find_element(By.CSS_SELECTOR, "span.text-green-500").text.replace("%", "").strip()
                except: pass
                
                try:
                    games = block.find_element(By.CSS_SELECTOR, "span.text-gray-400").text.replace(" Games", "").strip()
                except: pass

                # ìœ íš¨í•œ ì•„ì´í…œ ë°ì´í„°ë©´ ì¶”ê°€ (ì•„ì´í…œ IDê°€ 1000 ì´í•˜ë©´ ë³´í†µ ì¥ì‹ìš©ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ í•„í„°ë§ ê°€ëŠ¥í•˜ì§€ë§Œ ì¼ë‹¨ ìˆ˜ì§‘)
                items.append({
                    "id": item_id,
                    "win": win_rate,
                    "games": games
                })
            except:
                continue
                
    except Exception as e:
        pass
        
    return items

def crawl_builds():
    # ë¸Œë¼ìš°ì € ì˜µì…˜ ì„¤ì •
    options = Options()
    # options.add_argument("--headless") # ë””ë²„ê¹… ì‹œì—ëŠ” ì£¼ì„ ì²˜ë¦¬ (ë¸Œë¼ìš°ì € í™”ë©´ ë³´ì„)
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--log-level=3") # ë¶ˆí•„ìš”í•œ ë¡œê·¸ ìˆ¨ê¹€
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36")
    
    driver = webdriver.Chrome(options=options)
    champions = get_champion_list()
    all_data = {}

    print(f"ğŸš€ ì´ {len(champions)}ê°œ ì±”í”¼ì–¸ í¬ë¡¤ë§ ì‹œì‘...")

    for i, champ in enumerate(champions):
        slug = get_slug(champ)
        url = f"https://lolalytics.com/lol/{slug}/aram/build/"
        print(f"[{i+1}/{len(champions)}] {champ} ìˆ˜ì§‘ ì¤‘... ({slug})")

        try:
            driver.get(url)
            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° (ë„¤íŠ¸ì›Œí¬ ëŠë¦¬ë©´ ëŠ˜ë ¤ì£¼ì„¸ìš”)
            time.sleep(2.5) 

            # ë°ì´í„° ìˆ˜ì§‘
            build_data = {
                "starting": parse_starting_items(driver),  # ì‹œì‘ ì•„ì´í…œ ì „ìš©
                "core": parse_section(driver, "Core Build"),
                "item4": parse_section(driver, "Item 4"),
                "item5": parse_section(driver, "Item 5"),
                "item6": parse_section(driver, "Item 6"),
            }
            
            # ë°ì´í„°ê°€ ìœ ì˜ë¯¸í•˜ë©´ ì €ì¥
            if build_data["starting"] or build_data["core"]:
                all_data[champ] = build_data
                
        except Exception as e:
            print(f"âŒ {champ} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    driver.quit()

    # JSON íŒŒì¼ ì €ì¥
    # ì €ì¥ ê²½ë¡œ: backend/data/aram_builds.json
    save_dir = os.path.join("..", "backend", "data")
    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, "aram_builds.json")
    
    with open(save_path, "w", encoding="utf-8") as f:
        json.dump(all_data, f, indent=2, ensure_ascii=False)

    print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ! ì €ì¥ëœ íŒŒì¼: {os.path.abspath(save_path)}")

if __name__ == "__main__":
    crawl_builds()