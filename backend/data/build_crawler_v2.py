import time
import json
import os
import re
import requests
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options

# ==========================================
# 1. ì±”í”¼ì–¸ ì´ë¦„ ì˜ˆì™¸ ì²˜ë¦¬ (Lolalytics URL ê·œì¹™)
# ==========================================
URL_EXCEPTIONS = {
    "Renata": "renata",
    "MonkeyKing": "wukong",
    "Nunu": "nunu",
    "DrMundo": "drmundo",
    "JarvanIV": "jarvaniv",
    "LeeSin": "leesin",
    "MasterYi": "masteryi",
    "MissFortune": "missfortune",
    "TahmKench": "tahmkench",
    "TwistedFate": "twistedfate",
    "XinZhao": "xinzhao",
    "KogMaw": "kogmaw",
    "RekSai": "reksai",
    "Belveth": "belveth",
    "Glasc": "renata" # í˜¹ì‹œ ëª°ë¼ ì¶”ê°€
}

def get_champion_list():
    """ë¼ì´ì—‡ APIì—ì„œ ìµœì‹  ì±”í”¼ì–¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    try:
        ver_url = "https://ddragon.leagueoflegends.com/api/versions.json"
        version = requests.get(ver_url).json()[0]
        champ_url = f"https://ddragon.leagueoflegends.com/cdn/{version}/data/ko_KR/champion.json"
        data = requests.get(champ_url).json()
        return list(data['data'].keys())
    except:
        return ["Gwen", "Ezreal", "Ahri"] # ì‹¤íŒ¨ ì‹œ í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸

def get_slug(champ_id):
    if champ_id in URL_EXCEPTIONS: return URL_EXCEPTIONS[champ_id]
    return champ_id.lower().replace(" ", "").replace("'", "").replace(".", "")

def extract_id_from_url(url):
    """URLì—ì„œ ì•„ì´í…œ ID ìˆ«ìë§Œ ì¶”ì¶œ (ì˜ˆ: .../1001.webp -> 1001)"""
    if not url: return None
    match = re.search(r'/(\d+)\.webp', url)
    return int(match.group(1)) if match else None

def parse_section(driver, header_text):
    """íŠ¹ì • í—¤ë”(ì˜ˆ: Starting Items) ì•„ë˜ì˜ ì•„ì´í…œê³¼ í†µê³„ë¥¼ ì¶”ì¶œ"""
    items = []
    try:
        # 1. í—¤ë” í…ìŠ¤íŠ¸ë¡œ í•´ë‹¹ ì„¹ì…˜(ì»¨í…Œì´ë„ˆ) ì°¾ê¸° (XPath ì‚¬ìš©)
        # "Starting Items"ë¼ëŠ” í…ìŠ¤íŠ¸ë¥¼ ê°€ì§„ divì˜ ìƒìœ„ ë¶€ëª¨ë“¤ ì¤‘ ì ì ˆí•œ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ìŒ
        xpath = f"//div[contains(text(), '{header_text}')]/ancestor::div[contains(@class, 'basis')]"
        container = driver.find_element(By.XPATH, xpath)
        
        # 2. í•´ë‹¹ ì»¨í…Œì´ë„ˆ ì•ˆì˜ ëª¨ë“  ì•„ì´í…œ ë¸”ë¡(ì´ë¯¸ì§€ê°€ ìˆëŠ” div) ì°¾ê¸°
        # text-center í´ë˜ìŠ¤ë¥¼ ê°€ì§„ div ì•ˆì— ì´ë¯¸ì§€ê°€ ìˆìŒ
        item_blocks = container.find_elements(By.CSS_SELECTOR, "div.text-center > div.overflow-hidden")

        for block in item_blocks:
            try:
                # ì´ë¯¸ì§€ ì¶”ì¶œ
                img = block.find_element(By.TAG_NAME, "img")
                src = img.get_attribute("src")
                item_id = extract_id_from_url(src)
                if not item_id: continue

                # í†µê³„ ì¶”ì¶œ (ì´ë¯¸ì§€ ë¸”ë¡ì˜ ë¶€ëª¨ì˜ í˜•ì œë‚˜ ìì‹ì—ì„œ ì°¾ê¸°)
                # êµ¬ì¡°ìƒ ì´ë¯¸ì§€ ë°”ë¡œ ì•„ë˜ë‚˜ ì˜†ì— í†µê³„ divê°€ ìˆìŒ
                # ìƒìœ„ ë¶€ëª¨(text-center)ë¡œ ì˜¬ë¼ê°€ì„œ í†µê³„ ì°¾ê¸°
                parent = block.find_element(By.XPATH, "..") 
                
                win_rate = ""
                games = ""
                
                try:
                    # ìŠ¹ë¥  (ì´ˆë¡ìƒ‰ í…ìŠ¤íŠ¸)
                    wr_elem = parent.find_element(By.CSS_SELECTOR, "span.text-green-500")
                    win_rate = wr_elem.text.replace("%", "").replace(" Win Rate", "").strip()
                except: pass

                try:
                    # ê²Œì„ ìˆ˜ (íšŒìƒ‰ í…ìŠ¤íŠ¸)
                    g_elem = parent.find_element(By.CSS_SELECTOR, "span.text-gray-400")
                    games = g_elem.text.replace(" Games", "").strip()
                except: pass

                items.append({
                    "id": item_id,
                    "win": win_rate,
                    "games": games
                })
            except:
                continue
                
    except Exception as e:
        # í•´ë‹¹ ì„¹ì…˜ì´ ì—†ì„ ìˆ˜ë„ ìˆìŒ (ë¬´ì‹œ)
        pass
        
    return items

def crawl_builds():
    options = Options()
    # options.add_argument("--headless") # ë””ë²„ê¹…í•  ë• ì£¼ì„ ì²˜ë¦¬í•´ì„œ ë¸Œë¼ìš°ì € ëœ¨ëŠ” ê±° ë³´ì„¸ìš”
    options.add_argument("--window-size=1920,1080")
    options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/113.0.0.0 Safari/537.36")
    
    driver = webdriver.Chrome(options=options)
    champions = get_champion_list()
    all_data = {}

    print(f"ğŸš€ {len(champions)}ê°œ ì±”í”¼ì–¸ í¬ë¡¤ë§ ì‹œì‘...")

    for i, champ in enumerate(champions):
        slug = get_slug(champ)
        url = f"https://lolalytics.com/lol/{slug}/aram/build/"
        print(f"[{i+1}/{len(champions)}] {champ} -> {url}")

        try:
            driver.get(url)
            time.sleep(3) # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°

            build_data = {
                "starting": parse_section(driver, "Starting Items"),
                "core": parse_section(driver, "Core Build"),
                "item4": parse_section(driver, "Item 4"),
                "item5": parse_section(driver, "Item 5"),
                "item6": parse_section(driver, "Item 6"),
            }
            
            # ë°ì´í„°ê°€ ë¹„ì–´ìˆìœ¼ë©´ ì €ì¥ ì•ˆ í•¨
            if build_data["starting"] or build_data["core"]:
                all_data[champ] = build_data
                
        except Exception as e:
            print(f"âŒ Error {champ}: {e}")

    driver.quit()

    # JSON ì €ì¥
    save_path = os.path.join("..", "backend", "data", "aram_builds.json")
    os.makedirs(os.path.dirname(save_path), exist_ok=True) # í´ë” ì—†ìœ¼ë©´ ìƒì„±
    
    with open(save_path, "w", encoding="utf-8") as f:
        json.dump(all_data, f, indent=2, ensure_ascii=False)

    print(f"âœ… ì €ì¥ ì™„ë£Œ: {save_path}")

if __name__ == "__main__":
    crawl_builds()